/*
 *  (c) Copyright 2016-2017, 2021 Hewlett Packard Enterprise Development Company LP.
 *
 *  This software is available to you under a choice of one of two
 *  licenses. You may choose to be licensed under the terms of the
 *  GNU Lesser General Public License Version 3, or (at your option)
 *  later with exceptions included below, or under the terms of the
 *  MIT license (Expat) available in COPYING file in the source tree.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Lesser General Public License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  As an exception, the copyright holders of this Library grant you permission
 *  to (i) compile an Application with the Library, and (ii) distribute the
 *  Application containing code generated by the Library and added to the
 *  Application during this compilation process under terms of your choice,
 *  provided you also meet the terms and conditions of the Application license.
 *
 */


/*
 * TODO: Extend pointers with tags to prevent the ABA problem.
 * TODO: Use byte arrays for keys hashed to a 64-bit code (using cityhash)
 */

#include <cstring>

#include "nvmm/memory_manager.h"
#include "nvmm/heap.h"
#include "nvmm/fam.h"

#include "city.h"

#include "radixtree/common.h"
#include "radixtree/split_ordered.h"
#include "split_ordered_metrics.h"

namespace radixtree {

struct SplitOrderedList::Node {
    SplitOrderedList::ByteKey byte_key;
    size_t byte_key_size;
    SplitOrderedList::SoKey key;
    SplitOrderedList::Value value;
    TagGptr next;
};

struct SplitOrderedList::Descriptor {
    Gptr table;
    uint64_t count;
    uint64_t size;
};


typedef SplitOrderedList::SoKey so_key_t;

const int hard_max_buckets = 1024*1024;

#define MAX_LOAD     0.75

#define MSB (((uint64_t)1) << 63)
#define REVERSE_BYTE(x) ((so_key_t)((((((uint32_t)(x)) * 0x0802LU & 0x22110LU) | (((uint32_t)(x)) * 0x8020LU & 0x88440LU)) * 0x10101LU >> 16) & 0xff))
# define REVERSE(x) ((REVERSE_BYTE((((so_key_t)(x))) & 0xff) << 56) |       \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 8) & 0xff) << 48) |  \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 16) & 0xff) << 40) | \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 24) & 0xff) << 32) | \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 32) & 0xff) << 24) | \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 40) & 0xff) << 16) | \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 48) & 0xff) << 8) |  \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 56) & 0xff) << 0))

// Remove this line to disable cityhash
#define USE_CITYHASH

static inline uint64_t Hash64(const char *buf, size_t len)
{
#ifdef USE_CITYHASH
    return CityHash64(buf, len) & (~MSB);
#else
    return (uint64_t)strtoul(buf, NULL, 10) & (~MSB);
#endif
#if 0
    assert(len == 8);
    union {
        uint64_t u64;
        char buf[8];
    } tmp;
    memcpy(tmp.buf, buf, 8);
    return tmp.u64 & (~MSB);
#endif
}

static inline SplitOrderedList::SoKey so_regularkey(const SplitOrderedList::UKey lkey)
{
    return REVERSE(lkey | MSB);
}

static inline SplitOrderedList::SoKey so_dummykey(const SplitOrderedList::UKey lkey)
{
    return REVERSE(lkey);
}


/* 
 * Get bucket's parent by unsetting bucket's most significant 
 * turned-on bit.
 */
static inline uint64_t get_parent(uint64_t bucket)
{
    uint64_t t = bucket;

    t |= t >> 1;
    t |= t >> 2;
    t |= t >> 4;
    t |= t >> 8;
    t |= t >> 16;
    t |= t >> 32;     // creates a mask
    return bucket & (t >> 1);
}


static inline
Gptr atomic_compare_and_swap(void* target, size_t old_value, size_t new_value) {
    return fam_atomic_u64_compare_and_store((uint64_t*)target, (uint64_t)old_value, (uint64_t)new_value);
}

static inline
Gptr atomic_compare_and_swap(void* target, Gptr old_value, Gptr new_value) {
    return fam_atomic_u64_compare_and_store((uint64_t*)target, (uint64_t)old_value, (uint64_t)new_value);
}

static inline
TagGptr atomic_compare_and_swap(void* target, TagGptr old_value, TagGptr new_value) {
    TagGptr result;
    fam_atomic_128_compare_and_store((int64_t*)target, old_value.i64, new_value.i64, result.i64);
    return result;
}

static inline
uint64_t atomic_load(void* addr) {
    return fam_atomic_u64_read((uint64_t*)addr);
}

static inline
void atomic_load(void* addr, int64_t result[2]) {
    fam_atomic_128_read((int64_t*)addr, result);
}

static inline
void atomic_store(void* addr, uint64_t val) {
    fam_atomic_u64_write((uint64_t*)addr, val);
}

static inline
void atomic_store(void* addr, int64_t val[2]) {
    fam_atomic_128_write((int64_t*)addr, val);
}



static inline
uint64_t atomic_fetch_and_add(void* addr, uint64_t incr) {
    return fam_atomic_u64_fetch_and_add((uint64_t*)addr, incr);
}

template<typename T>
Gptr SplitOrderedList::toGlobal(T* ptr) {
  return mmgr_->LocalToGlobal((void*) ptr);
}

SplitOrderedList::Value
SplitOrderedList::ListFind (Eop& op, 
                      TagGptr *head_tgptr,
                      const ByteKey& byte_key,
                      const size_t byte_key_size,
                      SoKey key,
                      TagGptr **oprev_tgptr,
                      TagGptr *ocur_tgptr,
                      TagGptr *onext_tgptr)
{
    SplitOrderedList::SoKey ckey;
    Value cval;
    TagGptr *prev_tgptr = NULL;
    TagGptr  cur_tgptr;
    TagGptr  next_tgptr;

    int pointer_traversals;

    while (1) {
        prev_tgptr = head_tgptr;
        cur_tgptr = *prev_tgptr;
        pointer_traversals = 0;
        while (1) {
            Node* real_cur_tgptr = toLocal<Node>(cur_tgptr.gptr());
            if (real_cur_tgptr == NULL) {
                if (oprev_tgptr) { *oprev_tgptr = prev_tgptr; }
                if (ocur_tgptr) { *ocur_tgptr = cur_tgptr; }
                if (onext_tgptr) { *onext_tgptr = next_tgptr; }
                return 0;
            }
#ifdef PMEM
            fam_invalidate(real_cur_tgptr, sizeof(Node));
            next_tgptr = real_cur_tgptr->next;
            ckey = real_cur_tgptr->key;
            cval = real_cur_tgptr->value;
#else
            atomic_load(&real_cur_tgptr->next, next_tgptr.i64);
            ckey = atomic_load(&real_cur_tgptr->key);
#endif
            if (*prev_tgptr != TagGptr(0, cur_tgptr.gptr(), cur_tgptr.tag())) {
                break; // this means someone mucked with the list; start over
            }
            if (!next_tgptr.mark()) {  // if next pointer is not marked
                if (ckey >= key) { // if current key > key, the key isn't in the list; if current key == key, the key IS in the list
                    if (oprev_tgptr) { *oprev_tgptr = prev_tgptr; }
                    if (ocur_tgptr) { *ocur_tgptr = cur_tgptr; }
                    if (onext_tgptr) { *onext_tgptr = next_tgptr; }
                    if (ckey == key) {
                        fam_invalidate(real_cur_tgptr->byte_key, real_cur_tgptr->byte_key_size);
                        if (memcmp(real_cur_tgptr->byte_key, byte_key, real_cur_tgptr->byte_key_size) == 0) {
#ifndef PMEM
                            cval = atomic_load(&real_cur_tgptr->value);
#endif
                            METRIC_HISTOGRAM_UPDATE(metrics, pointer_traversal_, pointer_traversals);
                            return cval;
                        }
                        // if current byte_key != byte_key, the we don't know yet, keep looking
                    } else {
                        return 0;
                    }
                }
                // but if current key < key, the we don't know yet, keep looking
                prev_tgptr = &(real_cur_tgptr->next);
            } else {
                if (atomic_compare_and_swap(prev_tgptr, 
                                            TagGptr(0, cur_tgptr.gptr(), cur_tgptr.tag()), 
                                            TagGptr(0, next_tgptr.gptr(), cur_tgptr.tag()+1)) ==
                    TagGptr(0, cur_tgptr.gptr(), cur_tgptr.tag())) 
                {
                    heap_->Free(op, cur_tgptr.gptr()); 
                } else {
                    break;
                }
            }
            cur_tgptr = next_tgptr;
            pointer_traversals++;
        }
    }
}


int SplitOrderedList::ListInsert(Eop& op, 
                           TagGptr *head_tgptr,
                           TagGptr node_tgptr,
                           TagGptr *ocur_tgptr)
{
    Node* node = toLocal<Node>(node_tgptr.gptr());
    SplitOrderedList::SoKey key = node->key;

    while (1) {
        TagGptr *lprev_tgptr;
        TagGptr cur_tgptr;

        if (ListFind(op, head_tgptr, node->byte_key, node->byte_key_size, key, &lprev_tgptr, &cur_tgptr, NULL) != 0) {   
            if (ocur_tgptr) { *ocur_tgptr = cur_tgptr; }
            return 0;
        }
        node = toLocal<Node>(node_tgptr.gptr());
        node->next = TagGptr(0, cur_tgptr.gptr(), 0);
        if (atomic_compare_and_swap(lprev_tgptr, 
                                    TagGptr(0, cur_tgptr.gptr(), cur_tgptr.tag()), 
                                    TagGptr(0, node_tgptr.gptr(), cur_tgptr.tag()+1)) == 
            TagGptr(0, cur_tgptr.gptr(), cur_tgptr.tag())) 
        {
            if (ocur_tgptr) { *ocur_tgptr = cur_tgptr; }
            return 1;
        }
    }
}

int SplitOrderedList::ListDelete(Eop& op, TagGptr *head_tgptr, const ByteKey& byte_key, const size_t byte_key_size, SoKey sokey, TagGptr* ocur_tgptr)
{
    while (1) {
        TagGptr *lprev_tgptr;
        TagGptr  lcur_tgptr;
        TagGptr  lnext_tgptr;

        if (ListFind(op, head_tgptr, byte_key, byte_key_size, sokey, &lprev_tgptr, &lcur_tgptr, &lnext_tgptr) == 0) { return 0; }
        if (atomic_compare_and_swap(&toLocal<Node>(lcur_tgptr.gptr())->next, 
                                    TagGptr(0, lnext_tgptr.gptr(), lnext_tgptr.tag()), 
                                    TagGptr(1, lnext_tgptr.gptr(), lnext_tgptr.tag()+1)) != 
            TagGptr(0, lnext_tgptr.gptr(), lnext_tgptr.tag())) 
        { 
            continue; 
        }
        if (atomic_compare_and_swap(lprev_tgptr, 
                                    TagGptr(0, lcur_tgptr.gptr(), lcur_tgptr.tag()), 
                                    TagGptr(0, lnext_tgptr.gptr(), lcur_tgptr.tag()+1)) == 
            TagGptr(0, lcur_tgptr.gptr(), lcur_tgptr.tag())) 
        {
            if (ocur_tgptr) {
                *ocur_tgptr = lcur_tgptr;
            } else {
                heap_->Free(op, lcur_tgptr.gptr());
            }
        } else {
            ListFind(op, head_tgptr, byte_key, byte_key_size, sokey, NULL, NULL, NULL);
        }
        return 1;
    }
}


void
SplitOrderedList::InitializeBucket (Eop& op, uint64_t bucket)
{
    size_t parent = get_parent(bucket);
    TagGptr cur_tgptr;

    TagGptr* table = toLocal<TagGptr>(descriptor_->table);

    TagGptr tmp_tgptr;
    atomic_load(&table[parent], tmp_tgptr.i64);
    if (tmp_tgptr == TagGptr()) {
        InitializeBucket(op, parent);
    }
    Gptr dummy_gptr = heap_->Alloc(sizeof(Node)); 
    Node* dummy = toLocal<Node>(dummy_gptr);
    assert(dummy);
    dummy->byte_key_size = 0;
    dummy->key   = so_dummykey(bucket);
    dummy->value = 0;
    dummy->next  = TagGptr();
    fam_persist(dummy, sizeof(Node));
    TagGptr dummy_tgptr(0, dummy_gptr, 0);
    if (!ListInsert(op, &(table[parent]), dummy_tgptr, &cur_tgptr)) {
        heap_->Free(op, dummy_gptr);
        dummy_tgptr = cur_tgptr;
    }
    atomic_store(&table[bucket], TagGptr(0, dummy_tgptr.gptr(), dummy_tgptr.tag()).i64);
}


SplitOrderedList::SplitOrderedList(Mmgr *mmgr, Heap *heap, SplitOrderedMetrics* Metrics, Gptr descriptor_ptr)
    : mmgr_(mmgr), heap_(heap), metrics(Metrics), descriptor_ptr_(descriptor_ptr)
{
    assert(mmgr!=NULL);
    assert(heap!=NULL);

    if (descriptor_ptr == 0) { 
        descriptor_ptr_ = heap_->Alloc(sizeof(Descriptor));
        descriptor_ = toLocal<Descriptor>(descriptor_ptr_);
        descriptor_->count = 0;
        descriptor_->size = 2;
        descriptor_->table = heap_->Alloc(hard_max_buckets * sizeof(TagGptr));
        fam_persist(descriptor_, sizeof(*descriptor_));
 
        TagGptr* table = toLocal<TagGptr>(descriptor_->table);
    
        Gptr dummy_ptr = heap_->Alloc(sizeof(Node)); 
        Node* dummy = toLocal<Node>(dummy_ptr);
        dummy->key = so_dummykey(0);
        atomic_store(&table[0], TagGptr(0, dummy_ptr, 0).i64);
    } else {
        descriptor_ = toLocal<Descriptor>(descriptor_ptr_);
    }
}

SplitOrderedList::~SplitOrderedList() {}

Gptr SplitOrderedList::get_descriptor() {
    return descriptor_ptr_;
}

int SplitOrderedList::FindOrInsert(Eop& op, const ByteKey& byte_key, const size_t byte_key_size, SplitOrderedList::Value& value)
{
    Gptr node_gptr = heap_->Alloc(sizeof(Node));
    Node* node = toLocal<Node>(node_gptr);
    size_t bucket;
    uint64_t lkey = Hash64(byte_key, byte_key_size);

    bucket = lkey % descriptor_->size;

    assert(node);
    assert((lkey & MSB) == 0);
    memcpy(node->byte_key, byte_key, byte_key_size);
    node->byte_key_size = byte_key_size;
    node->key  = so_regularkey(lkey);
    node->value = value;
    node->next  = TagGptr();
    fam_persist(node, sizeof(*node));

    TagGptr* table = toLocal<TagGptr>(descriptor_->table);
    if (table[bucket] == TagGptr()) {
        InitializeBucket(op, bucket);
    }
    TagGptr cur_tgptr;
    TagGptr node_tgptr(0, node_gptr, 0);
    if (!ListInsert(op, &(table[bucket]), node_tgptr, &cur_tgptr)) {
        Node* cur_node = toLocal<Node>(cur_tgptr.gptr());
        value = cur_node->value;
        heap_->Free(op, node_gptr);
        return 0;
    }
    size_t csize = descriptor_->size;
    if (static_cast<double>(atomic_fetch_and_add(&descriptor_->count, 1)) / static_cast<double>(csize) > MAX_LOAD) {
        if (2 * csize <= hard_max_buckets) { // this caps the size of the hash
            atomic_compare_and_swap(&descriptor_->size, csize, 2 * csize);
        }
    }
    return 1;
}

int SplitOrderedList::Insert(Eop& op, const ByteKey& byte_key, const size_t byte_key_size, SplitOrderedList::Value value)
{
    Gptr node_gptr = heap_->Alloc(sizeof(Node));
    Node* node = toLocal<Node>(node_gptr);
    size_t bucket;
    uint64_t lkey = Hash64(byte_key, byte_key_size);

    bucket = lkey % descriptor_->size;

    assert(node);
    assert((lkey & MSB) == 0);
    memcpy(node->byte_key, byte_key, byte_key_size);
    node->byte_key_size = byte_key_size;
    node->key  = so_regularkey(lkey);
    node->value = value;
    node->next  = TagGptr();
    fam_persist(node, sizeof(*node));

    TagGptr* table = toLocal<TagGptr>(descriptor_->table);
    if (table[bucket] == TagGptr()) {
        InitializeBucket(op, bucket);
    }
    TagGptr node_tgptr(0, node_gptr, 0);
    if (!ListInsert(op, &(table[bucket]), node_tgptr, NULL)) {
        heap_->Free(op, node_gptr);
        return 0;
    }
    size_t csize = descriptor_->size;
    if (static_cast<double>(atomic_fetch_and_add(&descriptor_->count, 1)) / static_cast<double>(csize) > MAX_LOAD) {
        if (2 * csize <= hard_max_buckets) { // this caps the size of the hash
            atomic_compare_and_swap(&descriptor_->size, csize, 2 * csize);
        }
    }
    return 1;
}


int SplitOrderedList::InsertOrUpdate(Eop& op, const ByteKey& byte_key, const size_t byte_key_size, SplitOrderedList::Value value, Gptr* old_gptr)
{
    Gptr node_gptr = heap_->Alloc(sizeof(Node));
    Node* node = toLocal<Node>(node_gptr);
    size_t bucket;
    uint64_t lkey = Hash64(byte_key, byte_key_size);

    bucket = lkey % descriptor_->size;

    assert(node);
    assert((lkey & MSB) == 0);
    memcpy(node->byte_key, byte_key, byte_key_size);
    node->byte_key_size = byte_key_size;
    node->key  = so_regularkey(lkey);
    node->value = value;
    node->next  = TagGptr();
    fam_persist(node, sizeof(*node));

    TagGptr* table = toLocal<TagGptr>(descriptor_->table);
    if (table[bucket] == TagGptr()) {
        InitializeBucket(op, bucket);
    }
    TagGptr cur_tgptr;
    TagGptr node_tgptr(0, node_gptr, 0);
    if (!ListInsert(op, &(table[bucket]), node_tgptr, &cur_tgptr)) {
        *old_gptr = node_tgptr.gptr();
        Node* cur_node = toLocal<Node>(cur_tgptr.gptr());
        cur_node->value = value;
        return 1;
    } else {
        *old_gptr = 0;
    }
    size_t csize = descriptor_->size;
    if (static_cast<double>(atomic_fetch_and_add(&descriptor_->count, 1)) / static_cast<double>(csize) > MAX_LOAD) {
        if (2 * csize <= hard_max_buckets) { // this caps the size of the hash
            atomic_compare_and_swap(&descriptor_->size, csize, 2 * csize);
        }
    }
    return 1;
}

SplitOrderedList::Value SplitOrderedList::Find(Eop& op, const ByteKey& byte_key, const size_t byte_key_size)
{
    size_t   bucket;
    uint64_t lkey = Hash64(byte_key, byte_key_size);
    
    bucket = lkey % descriptor_->size;

    TagGptr* table = toLocal<TagGptr>(descriptor_->table);
    if (table[bucket] == TagGptr()) {
        InitializeBucket(op, bucket);
    }
    return ListFind(op, &(table[bucket]), byte_key, byte_key_size, so_regularkey(lkey), NULL, NULL, NULL);
}

int SplitOrderedList::Delete(Eop& op, const ByteKey& byte_key, const size_t byte_key_size, Gptr* ocur_ptr)
{
    size_t   bucket;
    uint64_t lkey = Hash64(byte_key, byte_key_size);

    bucket = lkey % descriptor_->size;

    TagGptr* table = toLocal<TagGptr>(descriptor_->table);
    if (table[bucket] == TagGptr()) {
        InitializeBucket(op, bucket);
    }
    TagGptr cur_tgptr;
    if (!ListDelete(op, &(table[bucket]), byte_key, byte_key_size, so_regularkey(lkey), &cur_tgptr)) {
        return 0;
    }
    if (ocur_ptr) {
        *ocur_ptr = cur_tgptr.gptr();
    }
    atomic_fetch_and_add(&descriptor_->count, -1);
    return 1;
}

void SplitOrderedList::foreach(std::function<void(SplitOrderedList*, SplitOrderedList::ByteKey, size_t, SplitOrderedList::Value)> f)
{
	TagGptr* table = toLocal<TagGptr>(descriptor_->table);
	TagGptr cur_tgptr = table[0];

	for (; cur_tgptr.gptr(); cur_tgptr = toLocal<Node>(cur_tgptr.gptr())->next) {
        Node* cur = toLocal<Node>(cur_tgptr.gptr());
        f(this, cur->byte_key, cur->byte_key_size, cur->value);
	}
}


} // namespace radixtree
