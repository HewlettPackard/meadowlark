/*
 *  (c) Copyright 2016-2017 Hewlett Packard Enterprise Development Company LP.
 *
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Lesser General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Lesser General Public License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  As an exception, the copyright holders of this Library grant you permission
 *  to (i) compile an Application with the Library, and (ii) distribute the
 *  Application containing code generated by the Library and added to the
 *  Application during this compilation process under terms of your choice,
 *  provided you also meet the terms and conditions of the Application license.
 *
 */

/*
 * TODO: Extend pointers with tags to prevent the ABA problem.
 * TODO: Use byte arrays for keys hashed to a 64-bit code (using cityhash)
 */

#include <cstring>

#include "nvmm/memory_manager.h"
#include "nvmm/heap.h"

#include "radixtree/split_list.h"
#include "radixtree/radixtree_libpmem.h"
#include "radixtree/radixtree_fam_atomic.h"

namespace radixtree {

struct SplitOrderedList::Node {
    SplitOrderedList::Key key;
    SplitOrderedList::Value value;
    Gptr next;
};

struct SplitOrderedList::Descriptor {
    Gptr table;
    unsigned int count;
    unsigned int size;
};

typedef SplitOrderedList::SoKey so_key_t;

const int hard_max_buckets = 1024;

#define MAX_LOAD     4

#define MARK_OF(x)           ((x) & 1)
#define PTR_MASK(x)          ((x) & ~(Gptr)1)
#define PTR_OF(x)            (PTR_MASK(x))
#define REAL_PTR_OF(x)            (toLocal<SplitOrderedList::Node>(PTR_MASK(x)))
#define CONSTRUCT(mark, ptr) (PTR_MASK((uintptr_t)ptr) | (mark))
#define UNINITIALIZED ((Gptr)0)

#define MSB (((uint64_t)1) << 63)
#define REVERSE_BYTE(x) ((so_key_t)((((((uint32_t)(x)) * 0x0802LU & 0x22110LU) | (((uint32_t)(x)) * 0x8020LU & 0x88440LU)) * 0x10101LU >> 16) & 0xff))
# define REVERSE(x) ((REVERSE_BYTE((((so_key_t)(x))) & 0xff) << 56) |       \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 8) & 0xff) << 48) |  \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 16) & 0xff) << 40) | \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 24) & 0xff) << 32) | \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 32) & 0xff) << 24) | \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 40) & 0xff) << 16) | \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 48) & 0xff) << 8) |  \
                     (REVERSE_BYTE((((so_key_t)(x)) >> 56) & 0xff) << 0))

# define HASH_KEY(key) key = qt_hashword(key)

/* this function based on http://burtleburtle.net/bob/hash/evahash.html */
# define rot(x, k) (((x) << (k)) | ((x) >> (32 - (k))))
static uint64_t qt_hashword(uint64_t key)
{  
    uint32_t a, b, c;

    const union {
        uint64_t key;
        uint8_t  b[sizeof(uint64_t)];
    } k = {
        key
    };

    a  = b = c = 0x32533d0c + sizeof(uint64_t); // an arbitrary value, randomly selected
    c += 47;

    b += k.b[7] << 24;
    b += k.b[6] << 16;
    b += k.b[5] << 8;
    b += k.b[4];
    a += k.b[3] << 24;
    a += k.b[2] << 16;
    a += k.b[1] << 8;
    a += k.b[0];

    c ^= b;
    c -= rot(b, 14);
    a ^= c;
    a -= rot(c, 11);
    b ^= a;
    b -= rot(a, 25);
    c ^= b;
    c -= rot(b, 16);
    a ^= c;
    a -= rot(c, 4);
    b ^= a;
    b -= rot(a, 14);
    c ^= b;
    c -= rot(b, 24);
    return ((uint64_t)c + (((uint64_t)b) << 32)) & (~MSB);
} 

static inline so_key_t so_regularkey(const key_t key)
{
    return REVERSE(key | MSB);
}

static inline so_key_t so_dummykey(const key_t key)
{
    return REVERSE(key);
}


/* 
 * Get bucket's parent by unsetting bucket's most significant 
 * turned-on bit.
 */
static inline uint64_t get_parent(uint64_t bucket)
{
    uint64_t t = bucket;

    t |= t >> 1;
    t |= t >> 2;
    t |= t >> 4;
    t |= t >> 8;
    t |= t >> 16;
    t |= t >> 32;     // creates a mask
    return bucket & (t >> 1);
}


static inline
Gptr atomic_compare_and_swap(void* target, Gptr old_value, Gptr new_value) {
    return fam_atomic_u64_compare_and_store((uint64_t*)target, (uint64_t)old_value, (uint64_t)new_value);
}

static inline
uint64_t atomic_load(void* addr) {
    return fam_atomic_u64_read((uint64_t*)addr);
}

static inline
void atomic_store(void* addr, uint64_t val) {
    fam_atomic_u64_write((uint64_t*)addr, val);
}

static inline
uint64_t atomic_fetch_and_add(void* addr, uint64_t incr) {
    return fam_atomic_u64_fetch_and_add((uint64_t*)addr, incr);
}

template<typename T>
Gptr SplitOrderedList::toGlobal(T* ptr) {
  return mmgr_->LocalToGlobal((void*) ptr);
}

SplitOrderedList::Value
SplitOrderedList::ListFind (Gptr  *head_ptr,
                      SoKey key,
                      Gptr **oprev_ptr,
                      Gptr  *ocur_ptr,
                      Gptr  *onext_ptr)
{
    so_key_t      ckey;
    Value cval;
    Gptr *prev_ptr = NULL;
    Gptr  cur_ptr  = UNINITIALIZED;
    Gptr  next_ptr = UNINITIALIZED;
    
    while (1) {
        prev_ptr = head_ptr;
        cur_ptr  = *prev_ptr;
        while (1) {
            if (REAL_PTR_OF(cur_ptr) == NULL) {
                if (oprev_ptr) { *oprev_ptr = prev_ptr; }
                if (ocur_ptr) { *ocur_ptr = cur_ptr; }
                if (onext_ptr) { *onext_ptr = next_ptr; }
                return 0;
            }
            next_ptr = REAL_PTR_OF(cur_ptr)->next;
            ckey = REAL_PTR_OF(cur_ptr)->key;
            cval = REAL_PTR_OF(cur_ptr)->value;
            if (*prev_ptr != CONSTRUCT(0, cur_ptr)) {
                break; // this means someone mucked with the list; start over
            }
            if (!MARK_OF(next_ptr)) {  // if next pointer is not marked
                if (ckey >= key) { // if current key > key, the key isn't in the list; if current key == key, the key IS in the list
                    if (oprev_ptr) { *oprev_ptr = prev_ptr; }
                    if (ocur_ptr) { *ocur_ptr = cur_ptr; }
                    if (onext_ptr) { *onext_ptr = next_ptr; }
                    return (ckey == key) ? cval : NULL;
                }
                // but if current key < key, the we don't know yet, keep looking
                prev_ptr = &(REAL_PTR_OF(cur_ptr)->next);
            } else {
                if (atomic_compare_and_swap(prev_ptr, CONSTRUCT(0, cur_ptr), CONSTRUCT(0, next_ptr)) == CONSTRUCT(0, cur_ptr)) {
                    heap_->Free(PTR_OF(cur_ptr)); // FIXME: use delayed free?
                } else {
                    break;
                }
            }
            cur_ptr = next_ptr;
        }
    }
}


int SplitOrderedList::ListInsert(Gptr *head_ptr,
                           Gptr node_ptr,
                           Gptr *ocur_ptr)
{
    Node* node = toLocal<Node>(node_ptr);
    so_key_t key = node->key;

    while (1) {
        Gptr *lprev_ptr;
        Gptr cur_ptr;

        if (ListFind(head_ptr, key, &lprev_ptr, &cur_ptr, NULL) != 0) {   
            if (ocur_ptr) { *ocur_ptr = cur_ptr; }
            return 0;
        }
        Node *node = REAL_PTR_OF(node_ptr);
        node->next = CONSTRUCT(0, cur_ptr);
        if (atomic_compare_and_swap(lprev_ptr, node->next, CONSTRUCT(0, node_ptr)) == CONSTRUCT(0, cur_ptr)) {
            if (ocur_ptr) { *ocur_ptr = cur_ptr; }
            return 1;
        }
    }

}

int SplitOrderedList::ListDelete(Gptr *head_ptr, SoKey key, Gptr* ocur_ptr)
{
    while (1) {
        Gptr *lprev_ptr;
        Gptr  lcur_ptr;
        Gptr  lnext_ptr;

        if (ListFind(head_ptr, key, &lprev_ptr, &lcur_ptr, &lnext_ptr) == NULL) { return 0; }
        if (atomic_compare_and_swap(&REAL_PTR_OF(lcur_ptr)->next, CONSTRUCT(0, lnext_ptr), CONSTRUCT(1, lnext_ptr)) != CONSTRUCT(0, lnext_ptr)) { continue; }
        if (atomic_compare_and_swap(lprev_ptr, CONSTRUCT(0, lcur_ptr), CONSTRUCT(0, lnext_ptr)) == CONSTRUCT(0, lcur_ptr)) {
            if (ocur_ptr) {
                *ocur_ptr = lcur_ptr;
            } else {
                heap_->Free(PTR_OF(lcur_ptr));
            }
        } else {
            ListFind(head_ptr, key, NULL, NULL, NULL);
        }
        return 1;
    }
}


void
SplitOrderedList::InitializeBucket (uint64_t bucket)
{
    size_t parent = get_parent(bucket);
    Gptr cur_ptr;

    Gptr* table = toLocal<Gptr>(descriptor_->table);

    if (atomic_load(&table[parent]) == UNINITIALIZED) {
        InitializeBucket(parent);
    }
    Gptr dummy_ptr = heap_->Alloc(sizeof(Node)); 
    Node* dummy = toLocal<Node>(dummy_ptr);
    assert(dummy);
    dummy->key   = so_dummykey(bucket);
    dummy->value = NULL;
    dummy->next  = UNINITIALIZED;
    if (!ListInsert(&(table[parent]), dummy_ptr, &cur_ptr)) {
        heap_->Free(PTR_OF(dummy_ptr));
        dummy_ptr = cur_ptr;
    }
    atomic_store(&table[bucket], CONSTRUCT(0, dummy_ptr));
}


SplitOrderedList::SplitOrderedList(Mmgr *mmgr, Heap *heap, Gptr descriptor_ptr)
    : mmgr_(mmgr), heap_(heap), descriptor_ptr_(descriptor_ptr)
{
    assert(mmgr!=NULL);
    assert(heap!=NULL);

    if (descriptor_ptr == 0) { 
        descriptor_ptr_ = heap_->Alloc(sizeof(Descriptor));
        descriptor_ = toLocal<Descriptor>(descriptor_ptr_);
        descriptor_->count = 0;
        descriptor_->size = 2;
        descriptor_->table = heap_->Alloc(1024 * sizeof(Gptr));
 
        Gptr* table = toLocal<Gptr>(descriptor_->table);
    
        Gptr dummy_ptr = heap_->Alloc(sizeof(Node)); 
        Node* dummy = toLocal<Node>(dummy_ptr);
        dummy->key = so_dummykey(0);
        atomic_store(&table[0], CONSTRUCT(0, dummy_ptr));
    } else {
        descriptor_ = toLocal<Descriptor>(descriptor_ptr_);
    }
}

SplitOrderedList::~SplitOrderedList() {}

Gptr SplitOrderedList::get_descriptor() {
    return descriptor_ptr_;
}

int SplitOrderedList::Insert(const Key& key, SplitOrderedList::Value value)
{
    Gptr node_ptr = heap_->Alloc(sizeof(Node));
    Node* node = toLocal<Node>(node_ptr);
    size_t      bucket;
    uint64_t    lkey = (uint64_t)key;

    HASH_KEY(lkey);
    bucket = lkey % descriptor_->size;

    assert(node);
    assert((lkey & MSB) == 0);
    node->key   = so_regularkey(lkey);
    node->value = value;
    node->next  = UNINITIALIZED;

    Gptr* table = toLocal<Gptr>(descriptor_->table);
    if (table[bucket] == UNINITIALIZED) {
        InitializeBucket(bucket);
    }
    if (!ListInsert(&(table[bucket]), node_ptr, NULL)) {
        heap_->Free(PTR_OF(node_ptr));
        return 0;
    }
    size_t csize = descriptor_->size;
    if (atomic_fetch_and_add(&descriptor_->count, 1) / csize > MAX_LOAD) {
        if (2 * csize <= hard_max_buckets) { // this caps the size of the hash
            atomic_compare_and_swap(&descriptor_->size, csize, 2 * csize);
        }
    }
    return 1;
}

SplitOrderedList::Value SplitOrderedList::Find(const SoKey& key)
{
    size_t   bucket;
    uint64_t lkey = (uint64_t)(uintptr_t)key;

    HASH_KEY(lkey);
    bucket = lkey % descriptor_->size;

    Gptr* table = toLocal<Gptr>(descriptor_->table);
    if (table[bucket] == UNINITIALIZED) {
        InitializeBucket(bucket);
    }
    return ListFind(&(table[bucket]), so_regularkey(lkey), NULL, NULL, NULL);
}

int SplitOrderedList::Delete(const SoKey& key, Gptr* ocur_ptr)
{
    size_t   bucket;
    uint64_t lkey = (uint64_t)(uintptr_t)key;

    HASH_KEY(lkey);
    bucket = lkey % descriptor_->size;

    Gptr* table = toLocal<Gptr>(descriptor_->table);
    if (table[bucket] == UNINITIALIZED) {
        InitializeBucket(bucket);
    }
    if (!ListDelete(&(table[bucket]), so_regularkey(lkey), ocur_ptr)) {
        return 0;
    }
    atomic_fetch_and_add(&descriptor_->count, -1);
    return 1;
}

void SplitOrderedList::foreach(void (*f)(SplitOrderedList* so, SplitOrderedList::Key, SplitOrderedList::Value))
{
	int i;
	Gptr* table = toLocal<Gptr>(descriptor_->table);
	Gptr cur_ptr = table[0];

	for (; cur_ptr; cur_ptr = REAL_PTR_OF(cur_ptr)->next) {
		/*if node_t::next bit is set the current node has been deleted*/
		//if (get_bit (cur->next) || !cur->value || is_dummy_node (cur->hash_code))
		//	continue;
        Node* cur = REAL_PTR_OF(cur_ptr);
        f(this, so_regularkey(cur->key), cur->value);
	}
}


} // namespace radixtree
